from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType
from datetime import datetime

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Gold Users Analysis") \
    .getOrCreate()

# Define schemas for tables
goldusers_signup_schema = StructType([
    StructField("userid", IntegerType(), True),
    StructField("gold_signup_date", DateType(), True)
])

users_schema = StructType([
    StructField("userid", IntegerType(), True),
    StructField("signup_date", DateType(), True)
])

sales_schema = StructType([
    StructField("userid", IntegerType(), True),
    StructField("created_date", DateType(), True),
    StructField("product_id", IntegerType(), True)
])

product_schema = StructType([
    StructField("product_id", IntegerType(), True),
    StructField("product_name", StringType(), True),
    StructField("price", IntegerType(), True)
])

# Define data for tables
goldusers_signup_data = [(1, datetime.strptime('2017-09-22', '%Y-%m-%d')),
                         (3, datetime.strptime('2017-04-21', '%Y-%m-%d'))]

users_data = [(1, datetime.strptime('2014-09-02', '%Y-%m-%d')),
              (2, datetime.strptime('2015-01-15', '%Y-%m-%d')),
              (3, datetime.strptime('2014-04-11', '%Y-%m-%d'))]

sales_data = [(1, datetime.strptime('2017-04-18', '%Y-%m-%d'), 2),
              (3, datetime.strptime('2019-12-18', '%Y-%m-%d'), 1),
              (2, datetime.strptime('2019-12-18', '%Y-%m-%d'), 3),
              (1, datetime.strptime('2019-10-23', '%Y-%m-%d'), 2),
              (1, datetime.strptime('2018-03-19', '%Y-%m-%d'), 3),
              (3, datetime.strptime('2016-12-20', '%Y-%m-%d'), 2),
              (1, datetime.strptime('2016-11-09', '%Y-%m-%d'), 1),
              (1, datetime.strptime('2016-05-20', '%Y-%m-%d'), 3),
              (2, datetime.strptime('2017-09-24', '%Y-%m-%d'), 1),
              (1, datetime.strptime('2017-03-11', '%Y-%m-%d'), 2),
              (1, datetime.strptime('2016-03-11', '%Y-%m-%d'), 1),
              (3, datetime.strptime('2016-11-10', '%Y-%m-%d'), 1),
              (3, datetime.strptime('2017-12-07', '%Y-%m-%d'), 2),
              (3, datetime.strptime('2016-12-15', '%Y-%m-%d'), 2),
              (2, datetime.strptime('2017-11-08', '%Y-%m-%d'), 2),
              (2, datetime.strptime('2018-09-10', '%Y-%m-%d'), 3)]

product_data = [(1, 'p1', 980), (2, 'p2', 870), (3, 'p3', 330)]

# Create DataFrames
goldusers_signup_df = spark.createDataFrame(goldusers_signup_data, schema=goldusers_signup_schema)
users_df = spark.createDataFrame(users_data, schema=users_schema)
sales_df = spark.createDataFrame(sales_data, schema=sales_schema)
product_df = spark.createDataFrame(product_data, schema=product_schema)

# Reviewing tables
goldusers_signup_df.show()
users_df.show()
sales_df.show()
product_df.show()

